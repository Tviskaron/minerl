"""
generate.py

Does some on-the-fly processing of univ.json and metadata.json. Splits the mp4 file
generated by render.py into different clips based on timestamps in metadata.json.

"""
import functools
from collections import OrderedDict
from typing import List, Optional, Tuple
import os
import pathlib
import sys
import time
import numpy
import tqdm
import subprocess
import json
import traceback

#######################
### UTILITIES
#######################
from minerl.data.util import Blacklist
from minerl.data.util.constants import (
    remove,
    J, E,
    EXP_MIN_LEN_TICKS,
    DATA_DIR,
    RENDER_DIR,
    BLACKLIST_TXT as BLACKLIST_PATH,
    GOOD_MARKER_NAME,
    FAILED_COMMANDS,
    GENERATE_VERSION,
    ThreadManager
)

# 1. Load the blacklist.
# Steven comment: There seem to be two different types of blacklists -- the
# BLACKLIST_PATH which is saved as a text file, and another, concurrency-safe
# Blacklist class which is based on a directory tree.
black_list = Blacklist()
# blacklist = set(numpy.loadtxt(BLACKLIST_PATH, dtype=numpy.str).tolist())


def format_seconds(ticks):
    """
    Given ticks (int) returns a string of format hour:minutes:seconds
    """
    seconds = ticks / 20
    hours = int(seconds / 3600)
    seconds = seconds - hours * 3600
    minutes = int(seconds / 60)
    seconds = seconds - minutes * 60
    seconds = round(seconds, 3)
    return str(hours) + ':' + str(minutes) + ':' + str(seconds)


def add_key_frames(inputPath, segments):
    keyframes = []
    for segment in segments:
        # Convert ticks into video FPS (don't use render ms!)
        keyframes.append(format_seconds(segment[3]))
        keyframes.append(format_seconds(segment[4]))
    split_cmd = ['ffmpeg', '-i', J(inputPath, 'recording.mp4'),
                 '-c:a', 'copy',
                 '-c:v', 'copy',
                 '-force_key_frames',
                 ','.join(keyframes), J(inputPath, 'keyframes_recording.mp4')]
    # print('Running: ' + ' '.join(split_cmd))

    try:
        subprocess.check_output(split_cmd, stderr=subprocess.STDOUT)
    except Exception as e:
        print('COMMAND FAILED:', e)
        print(split_cmd)
        FAILED_COMMANDS.append(split_cmd)


def extract_subclip(input_path, start_tick, stop_tick, output_name):
    split_cmd = ['ffmpeg', '-ss', format_seconds(start_tick), '-i',
                 J(input_path, 'keyframes_recording.mp4'), '-t', format_seconds(stop_tick - start_tick),
                 '-vcodec', 'copy', '-acodec', 'copy', '-y', output_name]
    # print('Running: ' + ' '.join(split_cmd))
    try:
        subprocess.check_output(split_cmd, stderr=subprocess.STDOUT)
    except Exception as e:
        print('COMMAND FAILED:', e)
        print(split_cmd)
        FAILED_COMMANDS.append(split_cmd)


def parse_metadata(start_marker, stop_marker):
    try:
        metadata = {}
        startMeta = start_marker['value']['metadata']
        endMeta = stop_marker['value']['metadata']
        metadata['start_position'] = start_marker['value']['position']
        metadata['end_position'] = stop_marker['value']['position']
        metadata['start_tick'] = startMeta['tick'] if 'tick' in startMeta else None
        metadata['end_tick'] = endMeta['tick'] if 'tick' in endMeta else None
        metadata['start_time'] = start_marker['realTimestamp']
        metadata['end_time'] = stop_marker['realTimestamp']

        # Recording the string sent to us by Minecraft server including experiment specific data like if we won or not
        metadata['server_info_str'] = endMeta['expMetadata']
        metadata['server_metadata'] = json.loads(
            endMeta['expMetadata'][endMeta['expMetadata'].find('experimentMetadata') + 19:-1])

        # Not meaningful in some older streams but included for completeness
        metadata['server_info_str_start'] = startMeta['expMetadata']
        metadata['server_metadata_start'] = json.loads(
            startMeta['expMetadata'][startMeta['expMetadata'].find('experimentMetadata') + 19:-1])

        # Record if player was in the winners list
        if 'players' in metadata['server_metadata'] and 'winners' in metadata['server_metadata']:
            metadata['success'] = metadata['server_metadata']['players'][0] in metadata['server_metadata']['winners']

        return metadata
    except Exception as e:
        traceback.print_exc()
        raise e


##################
### PIPELINE
#################

# 1. Get good and bad render paths.
# Also add bad renders discovered this time to blacklist.
def get_good_and_bad_recording_names(blacklist) -> Tuple[List[str], List[str]]:
    good_renders = []
    bad_renders = []
    for rendered_demo_dir in pathlib.Path(RENDER_DIR).iterdir():
        if not rendered_demo_dir.is_dir():
            print(f"Found nondirectory {rendered_demo_dir} inside {RENDER_DIR}, skipping...")

        recording_filename = rendered_demo_dir.name
        if recording_filename not in blacklist:
            if E(rendered_demo_dir):
                # Check if metadata has already been extracted.
                # If it has been computed see if it is valid
                # or not.
                if E(J(rendered_demo_dir, GOOD_MARKER_NAME)):
                    good_renders.append(recording_filename)
                else:
                    bad_renders.append(recording_filename)
                    blacklist.add(recording_filename)

    return good_renders, bad_renders


def _gen_sarsa_pairs(outputPath, manager, input, debug=False):
    n = manager.get_index()
    assert len(input) == 1
    recording_name = input[0]
    input_path = J(RENDER_DIR, recording_name)
    ret = gen_sarsa_pairs(outputPath, input_path, recording_name, lineNum=n, debug=debug)
    manager.free_index(n)
    return ret


def get_tick(ticks, ms):
    for i in range(len(ticks)):
        if ticks[i] >= ms:
            return i
    raise IndexError


# Finish conditions for gen_sarsa_pairs

def _treechop_finished(tick):
    gui = tick['slots']['gui']
    num_logs = 0
    if 'ContainerPlayer' in gui['type']:
        for slot in gui['slots']:
            # accounts for log and log2
            if slot and 'log' in slot['name']:
                num_logs += slot['count']
    return num_logs >= 64


def _treechop_adjust(univ, t):
    pass


def _o_iron_finished(tick):
    try:
        changes = tick['diff']['changes']
        for change in changes:
            if change['item'] == 'minecraft:iron_pickaxe' and change[
                'quantity_change'] > 0:
                return True
    except KeyError:
        pass
    return False


def _o_iron_adjust(univ, t):
    try:
        univ[t]['diff']['changes'] = [{
            'item': 'minecraft:iron_pickaxe', 'variant': 0, 'quantity_change': 1}]
    except KeyError:
        pass


def _o_dia_finished(tick):
    try:
        changes = tick['diff']['changes']
        for change in changes:
            if change['item'] == 'minecraft:diamond' and change['quantity_change'] > 0:
                return True
    except KeyError:
        pass
    return False


def _o_dia_adjust(univ, t):
    # print(univ[t])
    try:
        univ[t]['diff']['changes'] = [{
            'item': 'minecraft:diamond', 'variant': 0, 'quantity_change': 1}]
        # print(univ[t])
    except KeyError:
        pass


def _nav_finished(tick):
    try:
        for block in tick['touched_blocks']:
            if 'minecraft:diamond_block' in block['name']:
                return True
    except KeyError:
        pass
    return False


def _nav_adjust(univ, t):
    try:
        univ[t]['navigateHelper'] = 'minecraft:diamond_block'
    except KeyError:
        pass


finish_conditions = {
    'survivaltreechop': (_treechop_finished, _treechop_adjust),
    'o_iron': (_o_iron_finished, _o_iron_adjust),
    'o_dia': (_o_dia_finished, _o_dia_adjust),
    'navigate': (_nav_finished, _nav_adjust),
    'navigateextreme': (_nav_finished, _nav_adjust)
}

def _maybe_amend_exp_name(expName, univ_json, tick) -> Optional[str]:
    """For o_meat and o_bed experiments, returns a string if the expName should be amended.

    Otherwise, returns expName unchanged.
    """
    if expName == 'o_meat':
        # Look backwards for meat at most 32 ticks in the past
        # Lets players who were assigned obtain cooked X become winners for obtain cooked Y
        for i in range(32):
            if str(tick - i) in univ_json and 'slots' in univ_json[str(tick - i)]:
                slot = [elem.values() for elem in
                        univ_json[str(tick - i)]['slots']['gui']['slots']
                        if 'item.porkchopCooked' in elem.values()
                        or 'item.beefCooked' in elem.values()
                        or 'item.muttonCooked' in elem.values()]
                if len(slot) == 0:
                    continue
                if 'item.porkchopCooked' in slot[0]:
                    return expName + '/cooked_pork'
                if 'item.beefCooked' in slot[0]:
                    return expName + '/cooked_beef'
                if 'item.muttonCooked' in slot[0]:
                    return expName + '/cooked_mutton'
            else:
                break
    elif expName == 'o_bed':
        # Look backwards for a bed at most 32 ticks in the past
        # Lets players who were assigned obtain cooked X become winners for obtain cooked Y
        for i in range(32):
            if str(tick - i) in univ_json and 'slots' in univ_json[str(tick - i)]:
                slot = [elem.values() for elem in
                        univ_json[str(tick - i)]['slots']['gui']['slots']
                        if 'item.bed.black' in elem.values()
                        or 'item.bed.white' in elem.values()
                        or 'item.bed.yellow' in elem.values()]
                if len(slot) == 0:
                    continue
                if 'item.bed.black' in slot[0]:
                    return expName + '/black'
                if 'item.bed.yellow' in slot[0]:
                    return expName + '/yellow'
                if 'item.bed.white' in slot[0]:
                    return expName + '/white'
            else:
                break
    return expName


# 3. generate sarsa pairs
def gen_sarsa_pairs(outputPath, inputPath, recordingName, lineNum=None, debug=False):
    """
    Args:
        inputPath: The subdirectory of RENDER_DIR that contains the output of render.py
            for a single user replay. The mp4 file and actions data will be split into
            keyframes by this thing.
    """
    # Script to to pair actions with video recording
    # All times are in ms and we assume a actions list, a timestamp file, and a dis-syncronous mp4 video

    # Decide if absolute or relative (old format)
    # Disable data generation for old format
    if E(J(inputPath, 'metaData.json')):
        metadata = json.load(open(J(inputPath, 'metaData.json')))
        if 'generator' in metadata:
            version = metadata['generator'].split('-')[-2]
            if int(version) < 103:
                return 0
    else:
        tqdm.tqdm.write('No metadata in ' + inputPath)
        return 0

    # Generate recording segments
    # Sorted pairs of (start, stop, exprementName) timestamps (in ms)
    segments = []
    numNewSegments = 0

    markers_by_timestamp = OrderedDict()
    streamMetadata = json.load(open(J(inputPath, 'stream_meta_data.json')))

    if 'markers' in streamMetadata:
        markers_sp = streamMetadata['markers']
        for marker in markers_sp:
            markers_by_timestamp[marker['realTimestamp']] = marker
    else:
        if debug:
            print('No markers found in stream_meta_data.json! Was it over-writen on re-render?')
        return 0

    startTime = None
    startTick = None
    startMarker = None

    # If we have to load univ_json (large file) ensure we don't load it again.
    __univ_json = None
    def lazy_univ_json() -> dict:
        """Lazily load univ.json.

        Note that the returned dict is mutable --
        this property important for the adjustment functions in `finish_conditions` which
        modify univ_json."""
        nonlocal __univ_json
        if __univ_json is None:
            __univ_json = json.loads(open(J(inputPath, 'univ.json')).read())
        return __univ_json

    for marker_timestamp, marker in sorted(markers_by_timestamp.items()):
        expName = ""
        meta = None
        # Get experiment name (its a malformed json so we have to look it up by hand)
        if 'value' in marker and 'metadata' in marker['value'] and 'expMetadata' in marker['value']['metadata']:
            meta = marker['value']['metadata']

            malformedStr = meta['expMetadata']
            jsonThing = json.loads(malformedStr[malformedStr.find('experimentMetadata') + 19:-1])
            if 'experiment_name' in jsonThing and 'tick' in meta and meta.get('stopRecording'):
                expName = jsonThing['experiment_name']
                expName = _maybe_amend_exp_name(expName, lazy_univ_json(), meta['tick'])

                for finish_expName in finish_conditions.keys():
                    condition_fn, adjust_fn = finish_conditions[finish_expName]
                    if expName == finish_expName and startTick is not None:
                        cond_satisfied_ticks = []
                        metadata = parse_metadata(startMarker, marker)

                        # TODO these should be quit handlers that return success True/False
                        for i in range(min(400, meta['tick'] - startTick)):
                            considered_tick = (meta['tick'] - i)
                            try:
                                if condition_fn(lazy_univ_json()[str(considered_tick)]):
                                    cond_satisfied_ticks.append(considered_tick)
                            except KeyError:
                                pass

                        if cond_satisfied_ticks:
                            meta['tick'] = min(cond_satisfied_ticks[0])
                        else:
                            # Add change if winner
                            try:
                                server_metadata = metadata['server_metadata']
                                winners = server_metadata.get("winners")
                                if winners is not None and len(winners) > 0:
                                    adjust_fn(lazy_univ_json(), str(meta['tick']))
                            except (KeyError, TypeError) as e:
                                traceback.print_exc()
            else:
                continue

        if meta is not None:
            print(f'startRecording={meta.get("startRecording")} -- '
                  f'stopRecording={meta.get("stopRecording")}')
            if meta.get('startRecording') and 'tick' in meta:
                # If we encounter a start marker after a start marker there is an error and we should throw away this
                # previous start marker and start fresh
                startTime = marker_timestamp
                startTick = meta['tick']
                startMarker = marker

            if meta.get('stopRecording') and startTime is not None:
                segments.append((startMarker, marker, expName, startTick, meta['tick']))
                # segments.append((startTime, key, expName, startTick, meta['tick'], startMarker, marker))
                startTime = None
                startTick = None

    # Layout of segments (new)
    # 0.             1.            2.                3.           4.
    # Start Marker : Stop Marker : Experiment Name : Start Tick : Stop Tick

    # (hack patch)
    # 0.          1.         2.        3.          4.         5.             6
    # startTime : stopTime : expName : startTick : stopTick : startMarker :  stopMarker

    if not E(J(inputPath, "recording.mp4")):
        if debug:
            tqdm.tqdm.write('No recording found in ' + inputPath)
        return 0

    if len(markers_by_timestamp) == 0:
        if debug:
            tqdm.tqdm.write('No valid markers found')
        return 0

    if 'ticks' not in lazy_univ_json():
        if debug:
            tqdm.tqdm.write('No ticks file in ' + inputPath)
        return 0

    ticks = lazy_univ_json()['ticks']
    videoOffset_ms = streamMetadata['start_timestamp']
    videoOffset_ticks = get_tick(ticks, videoOffset_ms)

    segments = [(segment[0],
                 segment[1],
                 segment[2],
                 segment[3] - videoOffset_ticks,
                 segment[4] - videoOffset_ticks)
                for segment in segments]
    segments = [segment for segment in segments if segment[4] - segment[3] > EXP_MIN_LEN_TICKS and segment[3] > 0]

    pbar = tqdm.tqdm(total=len(segments), desc='Segments', leave=False, position=lineNum)

    if not segments or len(segments) == 0:
        if debug:
            tqdm.tqdm.write(f'No segments in {inputPath}')
        return 0
    try:
        if E(J(inputPath, 'keyframes_recording.mp4')):
            os.remove(J(inputPath, 'keyframes_recording.mp4'))
        add_key_frames(inputPath, segments)
    except subprocess.CalledProcessError as exception:
        open('errors.txt', 'a+').write(
            f"Error splitting {recordingName}:\033[0;31;47m {exception}        \033[0m 0;31;47m"
            + f"{inputPath}\n")
        return 0

    for pair in segments:
        time.sleep(0.05)
        startMarker = pair[0]
        stopMarker = pair[1]
        experimentName = pair[2]
        startTick = pair[3]
        stopTick = pair[4]

        # BAH introduce versioning
        experiment_id = 'g{}_{}'.format(GENERATE_VERSION, recordingName[len('player_stream_'):]) + "_" + str(
            int(startTick)) + '-' + str(int(stopTick))
        output_dir = pathlib.Path(outputPath, experimentName, experiment_id)
        video_output_path = output_dir / 'recording.mp4'
        univ_output_path = output_dir / 'univ.json'
        meta_output_path = output_dir / 'metadata.json'
        if not E(output_dir):
            os.makedirs(output_dir)
        if not (E(video_output_path) and E(univ_output_path) and E(meta_output_path)):
            try:
                # Remove potentially stale elements
                for file_path in [video_output_path, univ_output_path, meta_output_path]:
                    remove(file_path)

                json_to_write = {}
                for idx in range(startTick, stopTick + 1):
                    json_to_write[str(idx - startTick)] = lazy_univ_json()[str(idx)]

                # Split universal.json and metadata.json
                json.dump(json_to_write, open(univ_output_path, 'w'))
                json.dump(metadata, open(meta_output_path, 'w'))

                # Split video (without re-encoding)
                extract_subclip(inputPath, startTick, stopTick, video_output_path)

                numNewSegments += 1
                pbar.update(1)

            except KeyboardInterrupt:
                return numNewSegments
            except KeyError:
                open('errors.txt', 'a+').write(
                    "Key Error " + str(idx) + " not found in universal json: " + inputPath + '\n')
                continue
            except Exception as e:
                open('errors.txt', 'a+').write(
                    "Exception in segment rendering" + str(e) + str(type(e)) + inputPath + '\n')
                continue
    return numNewSegments


def main(parallel: bool = True, n_workers: int = 2):
    """
    The main render script.

    Args:
        parallel: If True, then use true multiprocessing to parallelize jobs. Otherwise,
            use multithreading which allows breakpoints and other debugging tools, but
            is slower.
    """

    print("Constructing data directory.")
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(RENDER_DIR, exist_ok=True)

    print("Retrieving metadata from files:")
    valid_renders, invalid_renders = get_good_and_bad_recording_names(black_list)
    print(len(valid_renders))
    print("... found {} valid recordings and {} invalid recordings"
          " out of {} total files".format(
        len(valid_renders), len(invalid_renders), len(os.listdir(RENDER_DIR)))
    )
    print("Rendering videos: ")
    numSegments = []
    if E('errors.txt'):
        os.remove('errors.txt')

    if parallel:
        import multiprocessing
        multiprocessing.freeze_support()
    else:
        import multiprocessing.dummy as multiprocessing

    super_not_parallel = True
    if super_not_parallel:
        # Not sure how they decide who is a valid render.
        for recording_name in valid_renders:
            # breakpoint()
            gen_sarsa_pairs(
                outputPath=DATA_DIR,
                recordingName=recording_name,
                inputPath=pathlib.Path(RENDER_DIR, recording_name),
                lineNum=0,
                debug=True)
            return

    try:
        with multiprocessing.Pool(n_workers, initializer=tqdm.tqdm.set_lock,
                                  initargs=(multiprocessing.RLock(),)) as pool:
            manager = ThreadManager(multiprocessing.Manager(), n_workers, 1, 1)
            func = functools.partial(_gen_sarsa_pairs, DATA_DIR, manager)
            numSegments = list(
                tqdm.tqdm(pool.imap_unordered(func, valid_renders), total=len(valid_renders), desc='Files', miniters=1,
                          position=0, maxinterval=1))

            # for recording_name, render_path in tqdm.tqdm(valid_renders, desc='Files'):
            #     numSegmentsRendered += gen_sarsa_pairs(render_path, recording_name, DATA_DIR)
    except Exception as e:
        print('\n' * n_workers)
        print("Exception in pool: ", type(e), e)
        print('Rendered {} new segments in total!'.format(sum(numSegments)))
        if E('errors.txt'):
            print('Errors:')
            print(open('errors.txt', 'r').read())
        return

    numSegmentsRendered = sum(numSegments)

    print('\n' * n_workers)
    print('Rendered {} new segments in total!'.format(numSegmentsRendered))
    if E('errors.txt'):
        print('Errors:')
        print(open('errors.txt', 'r').read())


def main_console():
    n_workers = int(sys.argv[1]) if len(sys.argv) > 1 else 2
    main(n_workers=n_workers)


if __name__ == "__main__":
    main_console()
